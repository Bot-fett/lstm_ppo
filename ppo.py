import pandas as pd
import numpy as np
import torch
import joblib
import os
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

# ==========================================
# [1] ì„¤ì • ì˜ì—­
# ==========================================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
DATA_DIR = os.path.join(BASE_DIR, "data")
MODEL_DIR = os.path.join(BASE_DIR, "models")
PPO_MODEL_DIR = os.path.join(BASE_DIR, "ppo_models")

if not os.path.exists(PPO_MODEL_DIR):
    os.makedirs(PPO_MODEL_DIR)

TICKERS = ['AAPL', 'AMD', 'AMZN', 'GOOGL', 'META', 'NVDA', 'PLTR', 'TSLA']
SEQ_LENGTH = 60
INITIAL_BALANCE = 10000 
TRANSACTION_FEE = 0.0005  # ìˆ˜ìˆ˜ë£Œ

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Feature ë¦¬ìŠ¤íŠ¸ (LSTMê³¼ ë™ì¼í•´ì•¼ í•¨)
FEATURES = [
    'Open', 'High', 'Low', 'Close', 'Volume', 
    'RSI', 'MACD', 'MACD_Signal', 'MA20', 
    'VWAP', 'ATR', 'News_Sentiment', 'Fear_Greed_Index', 'XLK'
]

# ==========================================
# [2] LSTM í´ë˜ìŠ¤ (êµ¬ì¡° ë™ì¼)
# ==========================================
class StockLSTM(torch.nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1):
        super(StockLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = torch.nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# ==========================================
# [3] ì£¼ì‹ ê±°ë˜ í™˜ê²½ (Gym)
# ==========================================
class StockTradingEnv(gym.Env):
    def __init__(self, df, lstm_model, scaler_X, scaler_y, initial_balance=10000, transaction_fee=0.0005):
        super(StockTradingEnv, self).__init__()
        
        self.df = df.reset_index(drop=True)
        self.lstm_model = lstm_model
        self.scaler_X = scaler_X
        self.scaler_y = scaler_y
        self.initial_balance = initial_balance
        self.transaction_fee = transaction_fee
        
        # Action: 0=ë§¤ë„, 1=ë³´ìœ , 2=ë§¤ìˆ˜
        self.action_space = spaces.Discrete(3)
        
        # Observation Space ì •ì˜
        # 1. LSTMì˜ˆì¸¡ìˆ˜ìµë¥ , 2. í˜„ì¬ìˆ˜ìµë¥ (ì „ë´‰ëŒ€ë¹„), 3. VWAPê´´ë¦¬ìœ¨, 4. RSI/100, 
        # 5. MACD, 6. ATR/Close(ë³€ë™ì„±ë¹„ìœ¨), 7. ì‹¬ë¦¬ì§€ìˆ˜, 8. ê³µí¬íƒìš•/100, 9. ë³´ìœ ë¹„ìœ¨, 10. í˜„ê¸ˆë¹„ìœ¨
        self.obs_dim = 10 
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(self.obs_dim,), dtype=np.float32
        )
        
        # Feature ì»¬ëŸ¼ ì¸ë±ì‹± ì¤€ë¹„
        self.feature_cols = [f for f in FEATURES if f in self.df.columns]
        
        self.current_step = SEQ_LENGTH
        self.balance = initial_balance
        self.shares_held = 0
        self.total_assets = initial_balance
        self.max_assets = initial_balance
        self.trades = []

    def reset(self, seed=None, options=None):
        if seed is not None:
            np.random.seed(seed)
        self.current_step = SEQ_LENGTH
        self.balance = self.initial_balance
        self.shares_held = 0
        self.total_assets = self.initial_balance
        self.max_assets = self.initial_balance
        self.trades = []
        return self._get_observation(), {}

    def _get_lstm_prediction(self):
        """LSTMì„ ì´ìš©í•´ 'ë‹¤ìŒ íƒ€ì„ìŠ¤í…ì˜ ì˜ˆìƒ ìˆ˜ìµë¥ ' ì˜ˆì¸¡"""
        if self.current_step < SEQ_LENGTH:
            return 0.0
        
        # LSTM ì…ë ¥ ë°ì´í„° ì¶”ì¶œ (SEQ_LENGTH ë§Œí¼)
        sequence = self.df[self.feature_cols].iloc[self.current_step - SEQ_LENGTH:self.current_step].values
        # ìŠ¤ì¼€ì¼ë§
        sequence_scaled = self.scaler_X.transform(sequence)
        
        with torch.no_grad():
            seq_tensor = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(device)
            # ì˜ˆì¸¡ëœ ìŠ¤ì¼€ì¼ëœ ìˆ˜ìµë¥ 
            pred_scaled = self.lstm_model(seq_tensor).cpu().numpy()[0, 0]
        
        # ì›ë˜ ìˆ˜ìµë¥  ìŠ¤ì¼€ì¼ë¡œ ë³µì›
        pred_return = self.scaler_y.inverse_transform([[pred_scaled]])[0, 0]
        return pred_return

    def _get_observation(self):
        # í˜„ì¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        row = self.df.iloc[self.current_step]
        prev_close = self.df.iloc[self.current_step - 1]['Close']
        
        # 1. LSTM ì˜ˆì¸¡ (ì˜ˆìƒ ìˆ˜ìµë¥ )
        predicted_return = self._get_lstm_prediction()
        
        # 2. í˜„ì¬ ë³€ë™ë¥  (ì „ë´‰ ëŒ€ë¹„)
        current_return = (row['Close'] - prev_close) / prev_close
        
        # 3. VWAP ê´´ë¦¬ìœ¨ (í˜„ì¬ê°€ê°€ VWAPë³´ë‹¤ ì–¼ë§ˆë‚˜ ë†’ëƒ/ë‚®ëƒ)
        vwap_diff = (row['Close'] - row['VWAP']) / row['VWAP'] if 'VWAP' in row else 0
        
        # 4. ê¸°íƒ€ ì§€í‘œ ì •ê·œí™”
        rsi_norm = row['RSI'] / 100.0 if 'RSI' in row else 0.5
        macd_val = row['MACD'] if 'MACD' in row else 0
        atr_ratio = (row['ATR'] / row['Close']) if 'ATR' in row else 0 # ê°€ê²© ëŒ€ë¹„ ë³€ë™ì„±
        sentiment = row['News_Sentiment'] if 'News_Sentiment' in row else 0
        fear_greed = row['Fear_Greed_Index'] / 100.0 if 'Fear_Greed_Index' in row else 0.5
        
        # 5. í¬íŠ¸í´ë¦¬ì˜¤ ìƒíƒœ (ì •ê·œí™”)
        total_val = self.balance + self.shares_held * row['Close']
        shares_ratio = (self.shares_held * row['Close']) / total_val # ìì‚° ì¤‘ ì£¼ì‹ ë¹„ì¤‘ (0~1)
        cash_ratio = self.balance / total_val # ìì‚° ì¤‘ í˜„ê¸ˆ ë¹„ì¤‘ (0~1)

        obs = np.array([
            predicted_return,
            current_return,
            vwap_diff,
            rsi_norm,
            macd_val,
            atr_ratio,
            sentiment,
            fear_greed,
            shares_ratio,
            cash_ratio
        ], dtype=np.float32)
        
        # NaN ë°©ì§€
        return np.nan_to_num(obs)

    def step(self, action):
        current_price = self.df.iloc[self.current_step]['Close']
        prev_assets = self.total_assets
        
        # í–‰ë™ ìˆ˜í–‰
        if action == 0:  # ë§¤ë„
            if self.shares_held > 0:
                sell_amount = self.shares_held * current_price * (1 - self.transaction_fee)
                self.balance += sell_amount
                self.trades.append({'action': 'SELL', 'price': current_price, 'step': self.current_step})
                self.shares_held = 0
                
        elif action == 2:  # ë§¤ìˆ˜
            if self.balance > current_price:
                max_shares = int(self.balance / (current_price * (1 + self.transaction_fee)))
                if max_shares > 0:
                    cost = max_shares * current_price * (1 + self.transaction_fee)
                    self.balance -= cost
                    self.shares_held += max_shares
                    self.trades.append({'action': 'BUY', 'price': current_price, 'step': self.current_step})

        # ìì‚° ê°±ì‹ 
        self.total_assets = self.balance + self.shares_held * current_price
        
        # ë³´ìƒ ê³„ì‚°: (í˜„ì¬ ìì‚° - ì´ì „ ìì‚°) / ì´ì „ ìì‚° * 100 (í¼ì„¼íŠ¸ ë‹¨ìœ„ ë³´ìƒ)
        reward = ((self.total_assets - prev_assets) / prev_assets) * 100
        
        # í˜ë„í‹°: ë„ˆë¬´ ë§¤ë§¤ë¥¼ ì•ˆí•˜ë©´(Holdë§Œ í•˜ë©´) ì•½ê°„ì˜ í˜ë„í‹°ë¥¼ ì£¼ì–´ ë§¤ë§¤ ìœ ë„
        # if action == 1:
        #     reward -= 0.001 

        self.current_step += 1
        done = self.current_step >= len(self.df) - 1
        truncated = False
        
        return self._get_observation(), reward, done, truncated, {'total_assets': self.total_assets}

# ==========================================
# [4] PPO í•™ìŠµ í•¨ìˆ˜
# ==========================================
def train_ppo(ticker):
    print(f"\nğŸ¤– [{ticker}] PPO ê°•í™”í•™ìŠµ ì‹œì‘...")
    
    # ë°ì´í„° ë¡œë“œ
    file_path = os.path.join(DATA_DIR, f"{ticker}_hourly_dataset.csv")
    if not os.path.exists(file_path):
        file_path = os.path.join(DATA_DIR, f"{ticker}_hourly_alp_yf_dataset_v2.csv") # íŒŒì¼ëª… ì£¼ì˜
        
    df = pd.read_csv(file_path)
    df.fillna(method='ffill', inplace=True)
    df.dropna(inplace=True)

    # LSTM ëª¨ë¸ ë¡œë“œ
    feature_cols = [f for f in FEATURES if f in df.columns]
    lstm_model = StockLSTM(input_size=len(feature_cols))
    lstm_model.load_state_dict(torch.load(f"{MODEL_DIR}/{ticker}_lstm.pth", map_location=device))
    lstm_model.to(device)
    lstm_model.eval()
    
    scaler_X = joblib.load(f"{MODEL_DIR}/{ticker}_scaler_X.pkl")
    scaler_y = joblib.load(f"{MODEL_DIR}/{ticker}_scaler_y.pkl")

    # í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬
    split_idx = int(len(df) * 0.8)
    train_df = df.iloc[:split_idx]
    val_df = df.iloc[split_idx:]

    # í•™ìŠµ/ê²€ì¦ ë°ì´í„° í¬ê¸° ì¶œë ¥
    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(train_df)}í–‰, ê²€ì¦ ë°ì´í„°: {len(val_df)}í–‰")
    
    # í™˜ê²½ ìƒì„±
    train_env = DummyVecEnv([lambda: StockTradingEnv(train_df, lstm_model, scaler_X, scaler_y, INITIAL_BALANCE, TRANSACTION_FEE)])
    val_env = StockTradingEnv(val_df, lstm_model, scaler_X, scaler_y, INITIAL_BALANCE, TRANSACTION_FEE)
    
    # ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ
    model = PPO("MlpPolicy", train_env, verbose=1, learning_rate=3e-4, batch_size=64, n_steps=2048)
    model.learn(total_timesteps=30000) # í•™ìŠµ íšŸìˆ˜ ì¡°ì ˆ ê°€ëŠ¥

    # ëª¨ë¸ ì €ì¥
    model.save(f"{PPO_MODEL_DIR}/{ticker}_ppo")
    print(f"âœ… PPO ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {ticker}")

    # ê²€ì¦
    print(f"\nğŸ” [{ticker}] ê²€ì¦ ì‹œì‘...")
    obs, _ = val_env.reset()
    done = False
    total_reward = 0

    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, truncated, info = val_env.step(action)
        total_reward += reward
        if truncated:
            break

    final_assets = info['total_assets']
    profit = final_assets - INITIAL_BALANCE
    profit_rate = (profit / INITIAL_BALANCE) * 100

    print(f"\n{'='*50}")
    print(f"[{ticker}] ê²€ì¦ ê²°ê³¼")
    print(f"{'='*50}")
    print(f"ì´ˆê¸° ìë³¸: ${INITIAL_BALANCE:,.2f}")
    print(f"ìµœì¢… ìì‚°: ${final_assets:,.2f}")
    print(f"ìˆ˜ìµ: ${profit:,.2f} ({profit_rate:.2f}%)")
    print(f"ì´ ë³´ìƒ: {total_reward:.4f}")
    print(f"ê±°ë˜ íšŸìˆ˜: {len(val_env.trades)}")
    print(f"{'='*50}\n")

    return {
        'ticker': ticker,
        'initial': INITIAL_BALANCE,
        'final': final_assets,
        'profit': profit,
        'profit_rate': profit_rate,
        'total_reward': total_reward,
        'trades': len(val_env.trades)
    }

if __name__ == "__main__":
    print("="*60)
    print("PPO ê°•í™”í•™ìŠµ ê¸°ë°˜ ì£¼ì‹ íŠ¸ë ˆì´ë”©")
    print("="*60)

    results = []

    for ticker in TICKERS:
        try:
            result = train_ppo(ticker)
            if result:
                results.append(result)
        except Exception as e:
            print(f"Error: {e}")
            import traceback
            traceback.print_exc()

    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    if results:
        print("\n" + "="*60)
        print("ì „ì²´ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        for r in results:
            print(f"{r['ticker']}: ${r['profit']:,.2f} ({r['profit_rate']:.2f}%), ê±°ë˜: {r['trades']}íšŒ")

        avg_profit_rate = np.mean([r['profit_rate'] for r in results])
        print(f"\ní‰ê·  ìˆ˜ìµë¥ : {avg_profit_rate:.2f}%")
        print("="*60)

        # ê²€ì¦ ê²°ê³¼ CSV ì €ì¥
        summary_df = pd.DataFrame(results)
        summary_path = os.path.join(PPO_MODEL_DIR, "ppo_train_summary.csv")
        summary_df.to_csv(summary_path, index=False)
        print(f"\nğŸ’¾ ê²€ì¦ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ: {summary_path}")