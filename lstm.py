import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
import joblib
import os

# ==========================================
# [1] ì„¤ì • ì˜ì—­
# ==========================================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
DATA_DIR = os.path.join(BASE_DIR, "data")
MODEL_DIR = os.path.join(BASE_DIR, "models")

if not os.path.exists(MODEL_DIR):
    os.makedirs(MODEL_DIR)

# ì‚¬ìš©í•  ì¢…ëª© ë¦¬ìŠ¤íŠ¸
TICKERS = ['AAPL', 'AMD', 'AMZN', 'GOOGL', 'META', 'NVDA', 'PLTR', 'TSLA']

# ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
SEQ_LENGTH = 60       
HIDDEN_SIZE = 64      
NUM_LAYERS = 2        
EPOCHS = 50           
BATCH_SIZE = 32       
LEARNING_RATE = 0.001 

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ í•™ìŠµ ì¥ì¹˜: {device}")

# í•™ìŠµì— ì‚¬ìš©í•  ë³€ìˆ˜ë“¤
FEATURES = [
    'Open', 'High', 'Low', 'Close', 'Volume', 
    'RSI', 'MACD', 'MACD_Signal', 'MA20', 
    'VWAP', 'ATR', 'News_Sentiment', 'Fear_Greed_Index', 'XLK'
]

# ==========================================
# [2] LSTM ëª¨ë¸ ì •ì˜
# ==========================================
class StockLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(StockLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :]) # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ê²°ê³¼
        return out

# ==========================================
# [3] í•™ìŠµ í•¨ìˆ˜
# ==========================================
def train_model(ticker):
    print(f"\nğŸ“¡ [{ticker}] ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    
    # 1. ë°ì´í„° ë¡œë“œ (V2 ë°ì´í„°ì…‹ ê¶Œì¥)
    # íŒŒì¼ëª…ì´ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ í™•ì¸ í•„ìš”
    file_path = os.path.join(DATA_DIR, f"{ticker}_hourly_dataset.csv") 
    
    # ë§Œì•½ V2 íŒŒì¼ëª…ì´ ë‹¤ë¥´ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í•˜ì„¸ìš”
    if not os.path.exists(file_path):
        file_path = os.path.join(DATA_DIR, f"{ticker}_hourly_alp_yf_dataset_v2.csv")

    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}")
        return

    df = pd.read_csv(file_path)
    df.fillna(method='ffill', inplace=True) # ê²°ì¸¡ì¹˜ ë³´ê°„
    df.dropna(inplace=True)

    # 2. íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ë¡œê·¸ ìˆ˜ìµë¥ )
    # ì ˆëŒ€ ê°€ê²©(Close)ì„ ë§ì¶”ëŠ” ê±´ ì–´ë µê¸°ë•Œë¬¸ì— ë³€í™”ìœ¨(Return)ì„ ë§ì¶¤
    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    df.dropna(inplace=True) # ìˆ˜ìµë¥  ê³„ì‚°ìœ¼ë¡œ ìƒê¸´ ì²« í–‰ NaN ì œê±°

    # Feature ì„ íƒ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§)
    available_features = [f for f in FEATURES if f in df.columns]
    print(f"âœ… ì‚¬ìš©ëœ Features: {available_features}")
    
    # X: ì—¬ëŸ¬ ê¸°ìˆ ì  ì§€í‘œë“¤, y: ë‹¤ìŒ ì‹œì ì˜ ë¡œê·¸ ìˆ˜ìµë¥ 
    X_data = df[available_features].values
    y_data = df[['Log_Return']].values

    # ======================================================
    # Data Leakage ë°©ì§€: Split í›„ Scaling
    # ======================================================
    split_idx = int(len(X_data) * 0.8)
    
    X_train_raw = X_data[:split_idx]
    X_val_raw = X_data[split_idx:]
    y_train_raw = y_data[:split_idx]
    y_val_raw = y_data[split_idx:]

    # ìŠ¤ì¼€ì¼ëŸ¬ ì •ì˜
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler() # yê°’(ìˆ˜ìµë¥ )ë„ ìŠ¤ì¼€ì¼ë§ ê¶Œì¥

    # Train ë°ì´í„°ë¡œë§Œ Fit
    X_train = scaler_X.fit_transform(X_train_raw)
    y_train = scaler_y.fit_transform(y_train_raw)

    # Val ë°ì´í„°ëŠ” Train ê¸°ì¤€ìœ¼ë¡œ Transform
    X_val = scaler_X.transform(X_val_raw)
    y_val = scaler_y.transform(y_val_raw)

    # 3. ì‹œê³„ì—´ ë°ì´í„°ì…‹ ìƒì„± (Sliding Window)
    def create_sequences(X, y, seq_length):
        xs, ys = [], []
        for i in range(len(X) - seq_length):
            xs.append(X[i : i+seq_length])
            ys.append(y[i+seq_length]) # ë‹¤ìŒ ì‹œì ì˜ ìˆ˜ìµë¥  ì˜ˆì¸¡
        return np.array(xs), np.array(ys)

    X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LENGTH)
    X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LENGTH)

    # í…ì„œ ë³€í™˜
    train_dataset = TensorDataset(
        torch.tensor(X_train_seq, dtype=torch.float32).to(device),
        torch.tensor(y_train_seq, dtype=torch.float32).to(device)
    )
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)

    X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)
    y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32).to(device)

    # 4. ëª¨ë¸ ì´ˆê¸°í™”
    model = StockLSTM(input_size=len(available_features), hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # 5. í•™ìŠµ ë£¨í”„
    best_loss = float('inf')
    patience = 0

    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor)
        
        avg_train_loss = train_loss / len(train_loader)
        
        if (epoch+1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss.item():.6f}")

        # Early Stopping check (ì„ íƒì‚¬í•­)
        if val_loss.item() < best_loss:
            best_loss = val_loss.item()
            patience = 0
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            torch.save(model.state_dict(), f"{MODEL_DIR}/{ticker}_lstm.pth")
        else:
            patience += 1
            
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ì˜ˆì¸¡ ì‹œ í•„ìˆ˜)
    joblib.dump(scaler_X, f"{MODEL_DIR}/{ticker}_scaler_X.pkl")
    joblib.dump(scaler_y, f"{MODEL_DIR}/{ticker}_scaler_y.pkl") # y ìŠ¤ì¼€ì¼ëŸ¬ë„ ì €ì¥
    
    print(f"âœ… {ticker} í•™ìŠµ ì™„ë£Œ. (Best Val Loss: {best_loss:.6f})")

if __name__ == "__main__":
    for ticker in TICKERS:
        try:
            train_model(ticker)
        except Exception as e:
            print(f"âš ï¸ {ticker} ì—ëŸ¬ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()